<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Topics in Trend Filtering with Poisson Loss</title>
    <meta charset="utf-8" />
    <meta name="author" content="Jiaping(Olivia) Liu" />
    <meta name="date" content="2023-05-30" />
    <script src="index_files/header-attrs/header-attrs.js"></script>
    <link rel="stylesheet" href="src/xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="src/slides-style.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">






layout: true

---

background-image: url("gfx/ptf-graph.png")
background-size: contain
background-position: bottom

.center[# Topics in Trend Filtering with Poisson Loss]

.pull-left[
#### Author &amp; Presenter: Jiaping(Olivia) Liu
#### Supervisor: Dr. Daniel J. McDonald
]

.pull-right[
#### Department of Statistics, UBC
#### June 13, 2023
]

&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;

---

## Trend filtering is locally adaptive.

Lobal adaptivity v.s. Global smoothness.

&lt;b&gt;Cubic trend filtering (&lt;span style="color: blue;"&gt;df=6&lt;/span&gt;)&lt;/b&gt; v.s. 
&lt;b&gt;Cubic smoothing splines (&lt;span style="color: orange;"&gt;df=6&lt;/span&gt; left; &lt;span style="color: orange;"&gt;df=20&lt;/span&gt; right) &lt;/b&gt;

&lt;div style="display: flex; justify-content: left;"&gt;
  &lt;img src="gfx/doppler6.png" width="1100" height="510"&gt;
&lt;/div&gt;


---

## Trend filtering is locally adaptive.

Beyond cubic trend filtering? Piecewise-polynomial.

&lt;div style="display: flex; justify-content: center;"&gt;
  &lt;img src="gfx/tfline.png" width=900" height="500"&gt;
&lt;/div&gt;

`\(\DeclareMathOperator*{\argmin}{argmin}\)`
`\(\DeclareMathOperator*{\Lambert}{Lambert_0}\)`
---

## Trend filtering is locally adaptive.

The `\(k\)`th degree _univariate_ trend filtering is defined as:
$$
\hat{\theta} = \underset{\theta \in \mathbb{R}^n }{\argmin} \frac{1}{2} {\left\lVert y - \theta \right\rVert}_2^2 + \lambda {\left\lVert \color{orange}{D^{(k+1)}} \theta \right\rVert}_1.
$$
- Divided difference matrix `\(\color{orange}{D^{(k+1)}}\)` constructs the polynomial curves.

- `\(\ell_1\)` norm introduces &lt;b&gt;sparsity&lt;/b&gt;.

--

&lt;span style="font-size: 30px;"&gt; **A simple example:** &lt;/span&gt;

- `\(D^{(1)}\)` and `\(D^{(2)}\)` are respectively

.pull-left[

```
##      [,1] [,2] [,3] [,4] [,5]
## [1,]   -1    1    0    0    0
## [2,]    0   -1    1    0    0
## [3,]    0    0   -1    1    0
## [4,]    0    0    0   -1    1
```
]
.pull-right[

```
##      [,1] [,2] [,3] [,4] [,5]
## [1,]    1   -2    1    0    0
## [2,]    0    1   -2    1    0
## [3,]    0    0    1   -2    1
```
]

- `\(|D^{(1)}\theta| = (|\theta_1-\theta_2|, ... , |\theta_4 - \theta_5|)\)` and `\(|D^{(2)}\theta| = (|\theta_1-2\theta_2 + \theta_3|, ..., |\theta_3 -2\theta_4 +\theta_5|)\)`.

---

## Trend filtering for Poisson data on graphs?

- Poisson data are:

  - counts, e.g., Covid-19 infection counts, pixels of images, and crime numbers.

  - heteroskedastic.
  
- Graphs, e.g., image grids, area maps.

.pull-left[
&lt;div style="display: flex; justify-content: right;"&gt;
  &lt;img src="gfx/kitty.jpg" width="400" height="270"&gt;
&lt;/div&gt;
]

.pull-right[
&lt;div style="display: flex; justify-content: left;"&gt;
  &lt;img src="gfx/graph.png" width="340" height="230"&gt;
&lt;/div&gt;
]

---

## Poisson trend filtering (PTF) on graphs 

TF with Poisson loss on graphs is defined as
`\begin{equation}
        \hat{\theta} = \underset{\theta \in \mathbb{R}^n }{\argmin} \frac{1}{n}\sum_{i=1}^n -y_i\theta_i + e^{\theta_i} + \lambda {\left\lVert \Delta^{(k+1)} \theta\right\rVert}_1.
\end{equation}`

--

&lt;span style="font-size: 25px;"&gt; **A simple example:** &lt;/span&gt;

.pull-left-narrow[
![graph](gfx/graph.png)
]

.pull-right-wide[
For example, `\(\Delta^{(1)}\)` and `\(\Delta^{(2)}\)` of the example graph are 
.pull-left[

```
##      [,1] [,2] [,3] [,4] [,5] [,6]
## [1,]   -1    1    0    0    0    0
## [2,]    0   -1    1    0    0    0
## [3,]    0    0   -1    1    0    0
## [4,]    0    0    0   -1    1    0
## [5,]    0    0    0   -1    0    1
## [6,]   -1    0    0    0    1    0
## [7,]    0   -1    0    0    1    0
```
]

.pull-right[

```
##      [,1] [,2] [,3] [,4] [,5] [,6]
## [1,]    2   -1    0    0   -1    0
## [2,]   -1    3   -1    0   -1    0
## [3,]    0   -1    2   -1    0    0
## [4,]    0    0   -1    3   -1   -1
## [5,]   -1   -1    0   -1    3    0
## [6,]    0    0    0   -1    0    1
```
]
]

- `\(|\Delta^{(1)}\theta| = (|\theta_1-\theta_2|, |\theta_2 - \theta_3|, ...)\)` and `\(|\Delta^{(2)}\theta| = (|\theta_1-\theta_2-\theta_5|, |-\theta_1 +3\theta_2 -\theta_3 -\theta_5|, ...)\)`. 

---

## Algorithms for PTF on graphs

- Generic Solvers (e.g., `\(\texttt{CVXR}\)`).

- Specialized algorithms for line graphs (e.g., `\(\texttt{glmgen}\)`).

--

- Our algorithms: 

&lt;div style="display: flex; justify-content: center;"&gt;
  &lt;img src="gfx/algo-map.png" width="860" height="430"&gt;
&lt;/div&gt;

---

## Algorithms for PTF on graphs

- Generic solvers (e.g., `\(\texttt{CVXR}\)`).

- Specialized algorithms for line graphs (e.g., `\(\texttt{glmgen}\)`).

- Our algorithms: ***&lt;span style="color: blue;"&gt;linearized alternating direction method of multipliers (linearized ADMM)&lt;/span&gt;*** and ***&lt;span style="color: blue;"&gt;proximal Newton method&lt;span&gt;***.

--

Approximation and decomposition. 

  - ***&lt;span style="color: blue;"&gt;linearized ADMM&lt;/span&gt;***: decomposition `\(\to\)` approximation;
  
  - ***&lt;span style="color: blue;"&gt;proximal Newton method&lt;span&gt;***: approximation `\(\to\)` decomposition.
  
--

&lt;div class="left"&gt;
&lt;h1 style="font-size:30px;"&gt;How can we choose between the two?&lt;/h1&gt;
&lt;/div&gt;

---
## Linearized ADMM for PTF on graphs

Let `\(z := \Delta^{(k+1)}\theta\)` and solve the scaled augmented Lagrangian 
`$$\mathcal{L}_{\lambda, \rho}(\theta, z, u) = \frac{1}{n} \sum_{i=1}^n -y_i \theta_i + e^{\theta_i} + \lambda {\left\lVert z \right\rVert}_1 + \frac{\rho}{2} {\left\lVert \Delta^{(k+1)}\theta - z + u \right\rVert}_2^2 - \frac{\rho}{2} {\left\lVert u \right\rVert}_2^2$$`
in three decomposed steps (for `\(\theta,z,u\)` sequentially) iteratively.

--

- At iterate `\(t+1\)`, the tricky part is `$$\theta^{t+1} := \underset{\theta\in \mathbb{R}^n}{\argmin} \frac{1}{n} \sum_{i=1}^n -y_i \theta_i + e^{\theta_i} + \frac{\rho}{2} {\left\lVert \Delta^{(k+1)}\theta - z^t + u^t \right\rVert}_2^2.$$`

--

- ***.stress[Linearization!]*** The linearized `\(\theta\)` step becomes `$$\theta^{t+1} := \underset{\theta\in \mathbb{R}^n}{\argmin} \frac{1}{n} \sum_{i=1}^n -y_i \theta_i + e^{\theta_i} + \rho \theta^{\top} (\Delta^{(k+1)})^{\top} (\Delta^{(k+1)} \theta^t - z^t + u^t) + \frac{ \mathbf{\color{orange}{\mu}}}{2} {\left\lVert \theta - \theta^t\right\rVert}_2^2,$$` where `\((\Delta^{(k+1)})^{\top}\Delta^{(k+1)}\)` is dominated by its largest eigenvalue.

---

## Proximal Newton method for PTF on graphs

At iterate `\(t+1\)`: 

- Approximate the Poisson loss and solve the *weighted Gaussian trend filtering* `$$\theta^{t_+} :=  \underset{\theta\in\mathbb{R}^n}{\arg\min} \frac{1}{2n} {\left\lVert \theta - c^{t} \right\rVert}_{W^t}^2 + \lambda {\left\lVert \Delta^{(k+1)}\theta \right\rVert}_1,$$` where `\(a^T W a := {\lVert a \rVert}_W^2\)`, `\(c^t\)` is the Gaussianized data and `\(W^t\)` is the weight. ***Solve it by ADMM.***

--

- Backtracking linesearch: `$${\theta}^{t+1} \leftarrow {\theta}^t + s^{t+1} ({\theta}^{t_+} - {\theta}^t),$$` where `\(s^{t+1}\)` is the step size.

---


## Comparisons of the two generic algorithms

  - ***&lt;span style="color: blue;"&gt;linearized ADMM&lt;/span&gt;***: decomposition `\(\to\)` approximation.
    
    - avoids matrix inversion;
    
    - usually requires many iterations.
  
  - ***&lt;span style="color: blue;"&gt;proximal Newton method&lt;span&gt;***: approximation `\(\to\)` decomposition.

    - requires a single-time matrix inversion for each iterate;

    - usually converges rapidly.

--

&lt;div class="left"&gt;
&lt;h1 style="font-size:30px;"&gt;Both have advantages and limitations. &lt;/h1&gt;
&lt;/div&gt;

---

## Empirical comparisons of the two generic algorithms

&lt;span style="font-size: 25px;"&gt; Problem designs: &lt;/span&gt;

- `\(3\)` degrees `\(k=0,1,2\)`.

- `\(3\)` different levels of regularization (using `\(\lambda_{\max}\)`)

  - low level: `\(10^{-4}\lambda_{\max}\)`; high level: `\(\lambda_{\max}\)`; medium level: their averages. 
  
- `\(20\)` different sizes of graph nodes in `\([20, 1000]\)`.

--

&lt;span style="font-size: 25px;"&gt; Random samples: &lt;/span&gt;

- `\(10\)` random samples. 

  - A unique graph structure for each random sample.

  - Random dot product graphs (RDPGs) with `\(10\%\)` non-sparsity. 
  
  - Each random sample is generated by `\(1000\)` inhomogeneous random walks with proper transformations. 

--

&lt;span style="font-size: 25px;"&gt; Computation: &lt;/span&gt; cold start for all experiments.

---

## Comparisons of running times

&lt;div style="display: flex; justify-content: center;"&gt;
  &lt;img src="gfx/runtime.png" width="1000" height="550"&gt;
&lt;/div&gt;

---

## Comparisons of running times

&lt;div style="display: flex; justify-content: center;"&gt;
  &lt;img src="gfx/single_iter_runtime.png" width="1000" height="550"&gt;
&lt;/div&gt;

---

## Comparisons of iteration numbers

&lt;div style="display: flex; justify-content: center;"&gt;
  &lt;img src="gfx/iter_num.png" width="1000" height="550"&gt;
&lt;/div&gt;

---

## Application in Epidemiology

- Daily infections `\(y_t \sim Pois(w_t \mathcal{R}_t)\)` on day `\(t\)`.

  - ***Effective reproduction number*** `\(\mathcal{R}_t\)` is the expected number of secondary infections caused by an infected individual in a population.
&lt;div style="display: flex; justify-content: center;"&gt;
  &lt;img src="gfx/rep-num.png" width="800" height="300"&gt;
&lt;/div&gt;

--
  - `\(w_t\)` is the weighted sum of infections prior to day `\(t\)`. 

--

- Estimate reproduction numbers `\(\mathcal{R}_t\)`? Temporal evolution of `\(\mathcal{R}_t\)`?

--

---
## Poisson trend filtering for reproduction number estimation

Let `\(\theta := \log(\mathcal{R})\)`. Given `\(w,y\)`, we define the model as 
`$$\hat{\theta} = \underset{\theta\in\mathbb{R}^n}{\argmin} \frac{1}{n}\sum_{i=1}^n -y_i \theta_i + w_i e^{\theta_i} + \lambda {\left\lVert D^{(k+1)} \theta \right\rVert}_1.$$`
--

- Given `\(y\)`, how the weighted previous counts `\(w\)` is calculated? 

  - Use serial interval function `\(\Phi\)`.
  
  - Choose a period of infection `\(\tau_{\Phi}\)`, and then `\(w_i = \sum_{j=1}^{\tau_{\Phi}} \Phi_j y_{i-j}, j=1,\cdots,\tau_{\Phi}\)`.
  
  - Approximate the probability `\(\Phi_j\)` by Gamma distributions.

---

## Covid-19 data application

Covid-19 daily confirmed counts between March 1st, 2020 and April 15th, 2023 in British Columbia, Canada. 
Data is available as of May 18, 2023 reported by B.C. Center of Disease Control. 

.pull-left-narrow[
If `\(\mathcal{R}\geq 1\)`,
the infections expand.

If `\(\mathcal{R}&lt; 1\)`,
the pandemic dies out.
]

.pull-right-wide[

![graph](gfx/covid19.png)
]

---


&lt;style&gt;
.center-vertically {
  display: flex;
  justify-content: center;
  align-items: center;
  height: 66vh;
}
&lt;/style&gt;

&lt;div class="center-vertically"&gt;
  &lt;h1 style="font-size: 66px;"&gt; Future work? &lt;/h1&gt;
&lt;/div&gt;


---

### _Future work 1:_ Explore theoretical results of trend filtering

The original exponential-family trend filtering problems on lattices:
`$$\hat{\theta} = \underset{\theta}{\mathrm{argmin}} \frac{1}{n} \sum_{i=1}^{n} -y_i\theta_i + \varphi(\theta_i) + \lambda {\left\lVert D^{[k+1]}_{n,d} \theta \right\rVert}_1,$$`
where `\([k+1] := (k_1+1,...,k_d+1)\)` is the degree for each row and column of the lattice.

--

- ***High excess prediction risk*** due to the lack of strong convexity.

- Existing approach can only converge in minimax rate when there is mild heteroskedasticity. 

--

- Our solution: weighted exponential-family trend filtering on lattices
`$$\hat{\theta}_{w} =\underset{\theta}{\mathrm{argmin}}\ \frac{1}{n} \sum_{i=1}^n -y_i\theta_i + \varphi(\theta_i) + \lambda {\left\lVert D^{[k+1]}_{n,d} W^{\circ} \theta \right\rVert}_1.$$`
  
  - `\(W^{\circ}\theta\)` returns standardized `\(\theta\)`, which eliminates heteroskedasticity.

  - Modify the problem to recover strong convexity. 
  
&lt;!--  - Show the estimator solves the problem and converges in minimax rate.
--&gt;

---

### _Future work 2:_ Connect optimization problems to state-space models

Consider a state-space model: 
`$$\begin{equation} 
    \begin{split}
        \text{observation equation: } y_i = \theta_i + \varepsilon_i,&amp; \ \varepsilon_i \overset{i.i.d.}{\sim} N\left(0, \sigma^2_{\varepsilon}\right), \\
        \text{state equation: } \theta_{i+1} = 2\theta_i - \theta_{i-1} + \zeta_{i},&amp; \ \zeta_i \overset{i.i.d.}{\sim} N\left(0, \sigma^2_{\zeta}\right),
    \end{split}
\end{equation}$$`

A corresponding optimization problem can be written as
`$$\underset{\theta}{\min} \frac{1}{2} {\left\lVert y - \theta\right\rVert}_2^2 + \lambda {\left\lVert D^{(2)} \theta \right\rVert}_2^2.$$`
--

Replace the normal by Laplace distribution in the state equation, it becomes 
`$$\underset{\theta}{\min} \frac{1}{2} {\left\lVert y - \theta\right\rVert}_2^2 + \lambda {\left\lVert D^{(2)} \theta \right\rVert}_1.$$`
- Motivations: 

  - State-space models provide a perspective of prediction. 
  - Algorithms for state-space models (e.g., _Kalman filter_) can solve optimization problems efficiently. 
  - Two fields are studied independently.

---
### _Future work 3:_ Parameter tuning for Poisson trend filtering

Poisson unbiased Kullback-Leibler (PUKL) estimator (proposed by _[deledalle2017estimation]_) can select the tuning parameters in Poisson trend filtering on lattices with null space projection: 
`$$\text{PUKL}(\hat{\theta}) = {\left\lVert \hat{\beta} \right\rVert}_1 - \langle y, \log \hat{\beta}_{\downarrow}(y) \rangle,$$`
where `\(\log \hat{\beta}_{\downarrow}(y) = \hat{\theta}_{\downarrow}(y), \{ \beta_{\downarrow}(y) \}_i = \{\beta(y - e_i)\}_i\)`, `\(e_i\)` is the `\(i\)`th standard basis vector.

- How does it work for Poisson trend filtering? 

---

## References

- [[Trend filtering on graphs]](https://www.jmlr.org/papers/volume17/15-147/15-147.pdf) _Wang, Y. X., Sharpnack, J., Smola, A. J., &amp; Tibshirani, R. J. (2016). Trend Filtering on Graphs. Journal of Machine Learning Research, 17, 1-41._
- [[Specialized ADMM for trend filtering on lines]](https://www.stat.cmu.edu/~ryantibs/papers/fasttf.pdf) _Ramdas, A., &amp; Tibshirani, R. J. (2016). Fast and flexible ADMM algorithms for trend filtering. Journal of Computational and Graphical Statistics, 25(3), 839-858._
- [[Proximal algorithms]](https://web.stanford.edu/~boyd/papers/pdf/prox_algs.pdf) _Parikh, N., &amp; Boyd, S. (2014). Proximal algorithms. Foundations and trends® in Optimization, 1(3), 127-239._
- [[Linearized ADMM]](https://www.ams.org/journals/mcom/2013-82-281/S0025-5718-2012-02598-1/S0025-5718-2012-02598-1.pdf) _Yang, J., &amp; Yuan, X. (2013). Linearized augmented Lagrangian and alternating direction methods for nuclear norm minimization. Mathematics of computation, 82(281), 301-329._
- [[Discrete splines]](https://www.nowpublishers.com/article/Details/MAL-099) _Tibshirani, R. J. (2022). Divided differences, falling factorials, and discrete splines: Another look at trend filtering and related problems. Foundations and Trends® in Machine Learning, 15(6), 694-846._
- [[Linearized ADMM]](https://www.jstor.org/stable/42002653) _Junfeng Yang and Xiaoming Yuan. Linearized augmented lagrangian and alternating direction methods for nuclear norm minimization. Mathematics of computation, 82(281): 301–329, 2013._
- [[Proximal Newtom method]](https://stanford.edu/group/SOL/multiscale/papers/14siopt-proxNewton.pdf) _Jason D Lee, Yuekai Sun, and Michael A Saunders. Proximal newton-type methods for minimizing composite functions. SIAM Journal on Optimization, 24(3):1420–1443, 2014._

---

class: center, middle

# Questions?

&lt;img src="gfx/unnamed-chunk-5-1.svg" width="100%" style="display: block; margin: auto;" /&gt;
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="src/macros.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"ratio": "16:9",
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
